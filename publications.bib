
@article{Kahn2017_StructuralPathwaysSupporting,
  title = {Structural {{Pathways Supporting Swift Acquisition}} of {{New Visuomotor Skills}}},
  volume = {27},
  issn = {1047-3211},
  url = {https://academic.oup.com/cercor/article/27/1/173/2632738},
  doi = {10.1093/cercor/bhw335},
  number = {1},
  journaltitle = {Cerebral Cortex},
  urldate = {2018-08-01},
  date = {2017-01-01},
  pages = {173-184},
  author = {Kahn, Ari E. and Mattar, Marcelo G. and Vettel, Jean M. and Wymbs, Nicholas F. and Grafton, Scott T. and Bassett, Danielle S.}
}

@article{Kahn2018_NetworkConstraintsLearnability,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.03000},
  langid = {english},
  title = {Network Constraints on Learnability of Probabilistic Motor Sequences},
  volume = {2},
  issn = {2397-3374},
  url = {https://www.nature.com/articles/s41562-018-0463-8},
  doi = {10.1038/s41562-018-0463-8},
  abstract = {Human learners are adept at grasping the complex relationships underlying incoming sequential input. In the present work, we formalize complex relationships as graph structures derived from temporal associations in motor sequences. Next, we explore the extent to which learners are sensitive to key variations in the topological properties inherent to those graph structures. Participants performed a probabilistic motor sequence task in which the order of button presses was determined by the traversal of graphs with modular, lattice-like, or random organization. Graph nodes each represented a unique button press and edges represented a transition between button presses. Results indicate that learning, indexed here by participants' response times, was strongly mediated by the graph's meso-scale organization, with modular graphs being associated with shorter response times than random and lattice graphs. Moreover, variations in a node's number of connections (degree) and a node's role in mediating long-distance communication (betweeness centrality) impacted graph learning, even after accounting for level of practice on that node. These results demonstrate that the graph architecture underlying temporal sequences of stimuli fundamentally constrains learning, and moreover that tools from network science provide a valuable framework for assessing how learners encode complex, temporally structured information.},
  number = {12},
  journaltitle = {Nature Human Behaviour},
  date = {2018-12},
  pages = {936-947},
  author = {Kahn, Ari E. and Karuza, Elisabeth A. and Vettel, Jean M. and Bassett, Danielle S.}
}

@article{Tompson2018_IndividualDifferencesLearning,
  langid = {english},
  title = {Individual Differences in Learning Social and Nonsocial Network Structures},
  issn = {1939-1285},
  doi = {10.1037/xlm0000580},
  abstract = {How do people acquire knowledge about which individuals belong to different cliques or communities? And to what extent does this learning process differ from the process of learning higher-order information about complex associations between nonsocial bits of information? Here, the authors use a paradigm in which the order of stimulus presentation forms temporal associations between the stimuli, collectively constituting a complex network. They examined individual differences in the ability to learn community structure of networks composed of social versus nonsocial stimuli. Although participants were able to learn community structure of both social and nonsocial networks, their performance in social network learning was uncorrelated with their performance in nonsocial network learning. In addition, social traits, including social orientation and perspective-taking, uniquely predicted the learning of social community structure but not the learning of nonsocial community structure. Taken together, the results suggest that the process of learning higher-order community structure in social networks is partially distinct from the process of learning higher-order community structure in nonsocial networks. The study design provides a promising approach to identify neurophysiological drivers of social network versus nonsocial network learning, extending knowledge about the impact of individual differences on these learning processes. (PsycINFO Database Record)},
  journaltitle = {Journal of Experimental Psychology. Learning, Memory, and Cognition},
  shortjournal = {J Exp Psychol Learn Mem Cogn},
  date = {2018-07-19},
  author = {Tompson, Steven H. and Kahn, Ari E. and Falk, Emily B. and Vettel, Jean M. and Bassett, Danielle S.},
  eprinttype = {pmid},
  eprint = {30024255}
}

@article{Lynn2018_StructureNoiseMental,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.12491},
  title = {Structure from Noise: {{Mental}} Errors Yield Abstract Representations of Events},
  url = {http://arxiv.org/abs/1805.12491},
  shorttitle = {Structure from Noise},
  abstract = {Humans are adept at uncovering abstract associations in the world around them, yet the underlying mechanisms remain poorly understood. Intuitively, learning the higher-order structure of statistical relationships should involve complex mental processes. Here we propose an alternative perspective: that higher-order associations instead arise from natural errors in learning and memory. Combining ideas from information theory and reinforcement learning, we derive a maximum entropy (or minimum complexity) model of people's internal representations of the transitions between stimuli. Importantly, our model (i) affords a concise analytic form, (ii) qualitatively explains the effects of transition network structure on human expectations, and (iii) quantitatively predicts human reaction times in probabilistic sequential motor tasks. Together, these results suggest that mental errors influence our abstract representations of the world in significant and predictable ways, with direct implications for the study and design of optimally learnable information sources.},
  journaltitle = {arXiv},
  urldate = {2019-03-03},
  date = {2018-05-31},
  keywords = {Quantitative Biology - Neurons and Cognition,Physics - Biological Physics,Physics - Physics and Society},
  author = {Lynn, Christopher W. and Kahn, Ari E. and Bassett, Danielle S.}
}

@article{Karuza2017_ProcessRevealsStructure,
  langid = {english},
  title = {Process Reveals Structure: {{How}} a Network Is Traversed Mediates Expectations about Its Architecture},
  volume = {7},
  issn = {2045-2322},
  url = {https://www.nature.com/articles/s41598-017-12876-5},
  doi = {10.1038/s41598-017-12876-5},
  shorttitle = {Process Reveals Structure},
  abstract = {Network science has emerged as a powerful tool through which we can study the higher-order architectural properties of the world around us. How human learners exploit this information remains an essential question. Here, we focus on the temporal constraints that govern such a process. Participants viewed a continuous sequence of images generated by three distinct walks on a modular network. Walks varied along two critical dimensions: their predictability and the density with which they sampled from communities of images. Learners exposed to walks that richly sampled from each community exhibited a sharp increase in processing time upon entry into a new community. This effect was eliminated in a highly regular walk that sampled exhaustively from images in short, successive cycles (i.e., that increasingly minimized uncertainty about the nature of upcoming stimuli). These results demonstrate that temporal organization plays an essential role in learners’ sensitivity to the network architecture underlying sensory input.},
  number = {1},
  journaltitle = {Scientific Reports},
  urldate = {2019-03-07},
  date = {2017-10-06},
  pages = {12733},
  author = {Karuza, Elisabeth A. and Kahn, Ari E. and Thompson-Schill, Sharon L. and Bassett, Danielle S.}
}

@article{Betzel2019_InterregionalECoGCorrelations,
  title = {Inter-Regional {{ECoG}} Correlations Predicted by Communication Dynamics, Geometry, and Correlated Gene Expression},
  abstract = {Electrocorticography (ECoG) provides direct measurements of synchronized postsynaptic potentials at the exposed cortical surface. Patterns of signal covariance across ECoG sensors have been associated with diverse cognitive functions and remain a critical marker of seizure onset, progression, and termination. Yet, a systems level understanding of these patterns (or networks) has remained elusive, in part due to variable electrode placement and sparse cortical coverage. Here, we address these challenges by constructing inter-regional ECoG networks from multi-subject recordings, demonstrate similarities between these networks and those constructed from blood-oxygen-level-dependent signal in functional magnetic resonance imaging, and predict network topology from anatomical connectivity, interregional distance, and correlated gene expression patterns. Our models accurately predict out-of-sample ECoG networks and perform well even when fit to data from individual subjects, suggesting shared organizing principles across persons. In addition, we identify a set of genes whose brain-wide co-expression is highly correlated with ECoG network organization. Using gene ontology analysis, we show that these same genes are enriched for membrane and ion channel maintenance and function, suggesting a molecular underpinning of ECoG connectivity. Our findings provide fundamental understanding of the factors that influence interregional ECoG networks, and open the possibility for predictive modeling of surgical outcomes in disease.},
  journaltitle = {Nature Biomedical Engineering},
  date = {2019},
  pages = {In Press},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Betzel, Richard F. and Medaglia, John D. and Kahn, Ari E. and Soffer, Jonathan and Schonhaut, Daniel R. and Bassett, Danielle S.}
}

@article{Baum2017_ModularSegregationStructural,
  langid = {english},
  title = {Modular {{Segregation}} of {{Structural Brain Networks Supports}} the {{Development}} of {{Executive Function}} in {{Youth}}},
  volume = {27},
  issn = {0960-9822},
  url = {https://www.cell.com/current-biology/abstract/S0960-9822(17)30496-7},
  doi = {10.1016/j.cub.2017.04.051},
  number = {11},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  urldate = {2019-03-09},
  date = {2017-06-05},
  pages = {1561-1572.e8},
  keywords = {MRI,network,brain,adolescence,connectome,development,DTI,executive,module,tractography},
  author = {Baum, Graham L. and Ciric, Rastko and Roalf, David R. and Betzel, Richard F. and Moore, Tyler M. and Shinohara, Russell T. and Kahn, Ari E. and Vandekar, Simon N. and Rupert, Petra E. and Quarmley, Megan and Cook, Philip A. and Elliott, Mark A. and Ruparel, Kosha and Gur, Raquel E. and Gur, Ruben C. and Bassett, Danielle S. and Satterthwaite, Theodore D.},
  eprinttype = {pmid},
  eprint = {28552358}
}

@article{Kim2018_RoleGraphArchitecture,
  langid = {english},
  title = {Role of Graph Architecture in Controlling Dynamical Networks with Applications to Neural Systems},
  volume = {14},
  issn = {1745-2481},
  url = {https://www.nature.com/articles/nphys4268},
  doi = {10.1038/nphys4268},
  abstract = {Networked systems display complex patterns of interactions between components. In physical networks, these interactions often occur along structural connections that link components in a hard-wired connection topology, supporting a variety of system-wide dynamical behaviours such as synchronization. Although descriptions of these behaviours are important, they are only a first step towards understanding and harnessing the relationship between network topology and system behaviour. Here, we use linear network control theory to derive accurate closed-form expressions that relate the connectivity of a subset of structural connections (those linking driver nodes to non-driver nodes) to the minimum energy required to control networked systems. To illustrate the utility of the mathematics, we apply this approach to high-resolution connectomes recently reconstructed from Drosophila, mouse, and human brains. We use these principles to suggest an advantage of the human brain in supporting diverse network dynamics with small energetic costs while remaining robust to perturbations, and to perform clinically accessible targeted manipulation of the brain’s control performance by removing single edges in the network. Generally, our results ground the expectation of a control system’s behaviour in its network architecture, and directly inspire new directions in network analysis and design via distributed control.},
  number = {1},
  journaltitle = {Nature Physics},
  urldate = {2019-03-09},
  date = {2018-01},
  pages = {91-98},
  author = {Kim, Jason Z. and Soffer, Jonathan M. and Kahn, Ari E. and Vettel, Jean M. and Pasqualetti, Fabio and Bassett, Danielle S.}
}

@article{Sizemore2018_CliquesCavitiesHuman,
  langid = {english},
  title = {Cliques and Cavities in the Human Connectome},
  volume = {44},
  issn = {1573-6873},
  url = {https://doi.org/10.1007/s10827-017-0672-6},
  doi = {10.1007/s10827-017-0672-6},
  abstract = {Encoding brain regions and their connections as a network of nodes and edges captures many of the possible paths along which information can be transmitted as humans process and perform complex behaviors. Because cognitive processes involve large, distributed networks of brain areas, principled examinations of multi-node routes within larger connection patterns can offer fundamental insights into the complexities of brain function. Here, we investigate both densely connected groups of nodes that could perform local computations as well as larger patterns of interactions that would allow for parallel processing. Finding such structures necessitates that we move from considering exclusively pairwise interactions to capturing higher order relations, concepts naturally expressed in the language of algebraic topology. These tools can be used to study mesoscale network structures that arise from the arrangement of densely connected substructures called cliques in otherwise sparsely connected brain networks. We detect cliques (all-to-all connected sets of brain regions) in the average structural connectomes of 8 healthy adults scanned in triplicate and discover the presence of more large cliques than expected in null networks constructed via wiring minimization, providing architecture through which brain network can perform rapid, local processing. We then locate topological cavities of different dimensions, around which information may flow in either diverging or converging patterns. These cavities exist consistently across subjects, differ from those observed in null model networks, and – importantly – link regions of early and late evolutionary origin in long loops, underscoring their unique role in controlling brain function. These results offer a first demonstration that techniques from algebraic topology offer a novel perspective on structural connectomics, highlighting loop-like paths as crucial features in the human brain’s structural architecture.},
  number = {1},
  journaltitle = {Journal of Computational Neuroscience},
  shortjournal = {J Comput Neurosci},
  urldate = {2019-03-09},
  date = {2018-02-01},
  pages = {115-145},
  keywords = {Network neuroscience,Applied topology,Persistent homology},
  author = {Sizemore, Ann E. and Giusti, Chad and Kahn, Ari and Vettel, Jean M. and Betzel, Richard F. and Bassett, Danielle S.}
}

@article{Tang2017_DevelopmentalIncreasesWhite,
  langid = {english},
  title = {Developmental Increases in White Matter Network Controllability Support a Growing Diversity of Brain Dynamics},
  volume = {8},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/s41467-017-01254-4},
  doi = {10.1038/s41467-017-01254-4},
  abstract = {Human brain development is characterized by an increased control of neural activity, but how this happens is not well understood. Here, authors show that white matter connectivity in 882 youth, aged 8-22, becomes increasingly specialized locally and is optimized for network control.},
  number = {1},
  journaltitle = {Nature Communications},
  urldate = {2019-03-09},
  date = {2017-11-01},
  pages = {1252},
  author = {Tang, Evelyn and Giusti, Chad and Baum, Graham L. and Gu, Shi and Pollock, Eli and Kahn, Ari E. and Roalf, David R. and Moore, Tyler M. and Ruparel, Kosha and Gur, Ruben C. and Gur, Raquel E. and Satterthwaite, Theodore D. and Bassett, Danielle S.}
}

@article{Gu2015_ControllabilityStructuralBrain,
  langid = {english},
  title = {Controllability of Structural Brain Networks},
  volume = {6},
  issn = {2041-1723},
  url = {https://www.nature.com/articles/ncomms9414},
  doi = {10.1038/ncomms9414},
  abstract = {Cognitive function is driven by dynamic interactions between large-scale neural circuits or networks, enabling behaviour. However, fundamental principles constraining these dynamic network processes have remained elusive. Here we use tools from control and network theories to offer a mechanistic explanation for how the brain moves between cognitive states drawn from the network organization of white matter microstructure. Our results suggest that densely connected areas, particularly in the default mode system, facilitate the movement of the brain to many easily reachable states. Weakly connected areas, particularly in cognitive control systems, facilitate the movement of the brain to difficult-to-reach states. Areas located on the boundary between network communities, particularly in attentional control systems, facilitate the integration or segregation of diverse cognitive systems. Our results suggest that structural network differences between cognitive circuits dictate their distinct roles in controlling trajectories of brain network function.},
  journaltitle = {Nature Communications},
  urldate = {2019-03-09},
  date = {2015-10-01},
  pages = {8414},
  author = {Gu, Shi and Pasqualetti, Fabio and Cieslak, Matthew and Telesford, Qawi K. and Yu, Alfred B. and Kahn, Ari E. and Medaglia, John D. and Vettel, Jean M. and Miller, Michael B. and Grafton, Scott T. and Bassett, Danielle S.}
}

@article{Karuza2019_HumanSensitivityCommunity,
  langid = {english},
  title = {Human {{Sensitivity}} to {{Community Structure Is Robust}} to {{Topological Variation}}},
  volume = {2019},
  issn = {1076-2787, 1099-0526},
  url = {https://www.hindawi.com/journals/complexity/2019/8379321/},
  doi = {10.1155/2019/8379321},
  journaltitle = {Complexity},
  urldate = {2019-03-10},
  date = {2019-02-11},
  pages = {1-8},
  author = {Karuza, Elisabeth A. and Kahn, Ari E. and Bassett, Danielle S.}
}

@article{Khambhati2019_FunctionalControlElectrophysiological,
  title = {Functional Control of Electrophysiological Network Architecture Using Direct Neurostimulation in Humans},
  url = {https://doi.org/10.1162/netn_a_00089},
  doi = {10.1162/netn_a_00089},
  abstract = {Chronically implantable neurostimulation devices are becoming a clinically viable option for treating patients with neurological disease and psychiatric disorders. Neurostimulation offers the ability to probe and manipulate distributed networks of interacting brain areas in dysfunctional circuits. Here, we use tools from network control theory to examine the dynamic reconfiguration of functionally interacting neuronal ensembles during targeted neurostimulation of cortical and subcortical brain structures. By integrating multimodal intracranial recordings and diffusion-weighted imaging from patients with drug-resistant epilepsy, we test hypothesized structural and functional rules that predict altered patterns of synchronized local field potentials. We demonstrate the ability to predictably reconfigure functional interactions depending on stimulation strength and location. Stimulation of areas with structurally weak connections largely modulates the functional hubness of downstream areas and concurrently propels the brain towards more difficult-to-reach dynamical states. By using focal perturbations to bridge large-scale structure, function, and markers of behavior, our findings suggest that stimulation may be tuned to influence different scales of network interactions driving cognition.},
  journaltitle = {Network Neuroscience},
  shortjournal = {Network Neuroscience},
  urldate = {2019-07-09},
  date = {2019-04-22},
  pages = {1-30},
  author = {Khambhati, Ankit N. and Kahn, Ari E. and Costantini, Julia and Ezzyat, Youssef and Solomon, Ethan A. and Gross, Robert E. and Jobst, Barbara C. and Sheth, Sameer A. and Zaghloul, Kareem A. and Worrell, Gregory and Seger, Sarah and Lega, Bradley C. and Weiss, Shennan and Sperling, Michael R. and Gorniak, Richard and Das, Sandhitsu R. and Stein, Joel M. and Rizzuto, Daniel S. and Kahana, Michael J. and Lucas, Timothy H. and Davis, Kathryn A. and Tracy, Joseph I. and Bassett, Danielle S.}
}

@article{Lynn2019_HumanInformationProcessingc,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.00926},
  primaryClass = {physics, q-bio},
  title = {Human Information Processing in Complex Networks},
  url = {http://arxiv.org/abs/1906.00926},
  abstract = {Humans communicate using systems of interconnected stimuli or concepts -- from language and music to literature and science -- yet it remains unclear how, if at all, the structure of these networks supports the communication of information. Although information theory provides tools to quantify the information produced by a system, traditional metrics do not account for the inefficient and biased ways that humans process this information. Here we develop an analytical framework to study the information generated by a system as perceived by a human observer. We demonstrate experimentally that this perceived information depends critically on a system's network topology. Applying our framework to several real networks, we find that they communicate a large amount of information (having high entropy) and do so efficiently (maintaining low divergence from human expectations). Moreover, we show that such efficient communication arises in networks that are simultaneously heterogeneous, with high-degree hubs, and clustered, with tightly-connected modules -- the two defining features of hierarchical organization. Together, these results suggest that many real networks are constrained by the pressures of information transmission, and that these pressures select for specific structural features.},
  urldate = {2019-07-09},
  date = {2019-06-03},
  keywords = {Quantitative Biology - Neurons and Cognition,Physics - Biological Physics,Physics - Physics and Society},
  author = {Lynn, Christopher W. and Papadopoulos, Lia and Kahn, Ari E. and Bassett, Danielle S.}
}

@article{Tompson2020_FunctionalBrainNetwork,
  langid = {english},
  title = {Functional Brain Network Architecture Supporting the Learning of Social Networks in Humans},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811919310894},
  doi = {10.1016/j.neuroimage.2019.116498},
  abstract = {Most humans have the good fortune to live their lives embedded in richly structured social groups. Yet, it remains unclear how humans acquire knowledge about these social structures to successfully navigate social relationships. Here we address this knowledge gap with an interdisciplinary neuroimaging study drawing on recent advances in network science and statistical learning. Specifically, we collected BOLD MRI data while participants learned the community structure of both social and non-social networks, in order to examine whether the learning of these two types of networks was differentially associated with functional brain network topology. We found that participants learned the community structure of the networks, as evidenced by a slower reaction time when a trial moved between communities than when a trial moved within a community. Learning the community structure of social networks was also characterized by significantly greater functional connectivity of the hippocampus and temporoparietal junction when transitioning between communities than when transitioning within a community. Furthermore, temporoparietal regions of the default mode were more strongly connected to hippocampus, somatomotor, and visual regions for social networks than for non-social networks. Collectively, our results identify neurophysiological underpinnings of social versus non-social network learning, extending our knowledge about the impact of social context on learning processes. More broadly, this work offers an empirical approach to study the learning of social network structures, which could be fruitfully extended to other participant populations, various graph architectures, and a diversity of social contexts in future studies.},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2020-01-13},
  date = {2020-01-07},
  eid = {116498},
  keywords = {Functional brain networks,Social cognition,Social network learning,Statistical learning},
  author = {Tompson, Steven H. and Kahn, Ari E. and Falk, Emily B. and Vettel, Jean M. and Bassett, Danielle S.}
}

@article{Stiso2019_WhiteMatterNetwork,
  title = {White {{Matter Network Architecture Guides Direct Electrical Stimulation}} through {{Optimal State Transitions}}},
  volume = {28},
  issn = {2211-1247},
  url = {http://www.sciencedirect.com/science/article/pii/S2211124719310411},
  doi = {10.1016/j.celrep.2019.08.008},
  abstract = {Summary
Optimizing direct electrical stimulation for the treatment of neurological disease remains difficult due to an incomplete understanding of its physical propagation through brain tissue. Here, we use network control theory to predict how stimulation spreads through white matter to influence spatially distributed dynamics. We test the theory’s predictions using a unique dataset comprising diffusion weighted imaging and electrocorticography in epilepsy patients undergoing grid stimulation. We find statistically significant shared variance between the predicted activity state transitions and the observed activity state transitions. We then use an optimal control framework to posit testable hypotheses regarding which brain states and structural properties will efficiently improve memory encoding when stimulated. Our work quantifies the role that white matter architecture plays in guiding the dynamics of direct electrical stimulation and offers empirical support for the utility of network control theory in explaining the brain’s response to stimulation.},
  number = {10},
  journaltitle = {Cell Reports},
  shortjournal = {Cell Reports},
  urldate = {2019-09-12},
  date = {2019-09-03},
  pages = {2554-2566.e7},
  keywords = {brain network,brain stimulation,network control theory},
  author = {Stiso, Jennifer and Khambhati, Ankit N. and Menara, Tommaso and Kahn, Ari E. and Stein, Joel M. and Das, Sandihitsu R. and Gorniak, Richard and Tracy, Joseph and Litt, Brian and Davis, Kathryn A. and Pasqualetti, Fabio and Lucas, Timothy H. and Bassett, Danielle S.}
}

@article{Corsi2020_FunctionalDisconnectionAssociative,
  langid = {english},
  title = {Functional Disconnection of Associative Cortical Areas Predicts Performance during {{BCI}} Training},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811919310912},
  doi = {10.1016/j.neuroimage.2019.116500},
  abstract = {Brain-computer interfaces (BCIs) have been largely developed to allow communication, control, and neurofeedback in human beings. Despite their great potential, BCIs perform inconsistently across individuals and the neural processes that enable humans to achieve good control remain poorly understood. To address this question, we performed simultaneous high-density electroencephalographic (EEG) and magnetoencephalographic (MEG) recordings in a motor imagery-based BCI training involving a group of healthy subjects. After reconstructing the signals at the cortical level, we showed that the reinforcement of motor-related activity during the BCI skill acquisition is paralleled by a progressive disconnection of associative areas which were not directly targeted during the experiments. Notably, these network connectivity changes reflected growing automaticity associated with BCI performance and predicted future learning rate. Altogether, our findings provide new insights into the large-scale cortical organizational mechanisms underlying BCI learning, which have implications for the improvement of this technology in a broad range of real-life applications.},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2020-01-13},
  date = {2020-01-09},
  eid = {116500},
  keywords = {Brain-computer interface,EEG,Learning,MEG,Motor imagery,Network},
  author = {Corsi, Marie-Constance and Chavez, Mario and Schwartz, Denis and George, Nathalie and Hugueville, Laurent and Kahn, Ari E. and Dupont, Sophie and Bassett, Danielle S. and De Vico Fallani, Fabrizio}
}
